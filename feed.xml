<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://yi-shiuan-tung.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yi-shiuan-tung.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-22T23:47:48+00:00</updated><id>https://yi-shiuan-tung.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Improving Human Legibility in Collaborative Robot Tasks through Augmented Reality and Workspace Preparation</title><link href="https://yi-shiuan-tung.github.io/blog/2023/human-legibility/" rel="alternate" type="text/html" title="Improving Human Legibility in Collaborative Robot Tasks through Augmented Reality and Workspace Preparation"/><published>2023-04-01T00:00:00+00:00</published><updated>2023-04-01T00:00:00+00:00</updated><id>https://yi-shiuan-tung.github.io/blog/2023/human-legibility</id><content type="html" xml:base="https://yi-shiuan-tung.github.io/blog/2023/human-legibility/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>The canonical approach for human robot interaction is to first predict the human motion and then generate a robot plan. This requires a robust and accurate predictions of human behavior. If the human models are inaccurate, the robot may produce unsafe interactions.</p> <p>To achieve more fluent human robot interaction, there has also been work on improving the robot’s expressiveness of its intentions. Dragan et al. developed a formalism for legibility which is the probability of successfully predicting an agent’s goal given an observation of a snippet of its trajectory <sup id="fnref:dragan_legibility" role="doc-noteref"><a href="#fn:dragan_legibility" class="footnote" rel="footnote">1</a></sup>. Subsequent works have shown that robots with legible movements lead to greater task efficiency, trustworthiness, and sense of safety in human robot collaboration. However, there hasn’t been work on looking at how human legibility can improve the robot’s ability to collaborate with humans.</p> <h2 id="our-approach">Our Approach</h2> <p>We improve the robot’s prediction of the human’s goal (aka the human’s legibility) by <strong>projecting virtual barriers in augmented reality and rearranging objects in the workspace</strong>. We present a metric that evaluates an environment configuration in terms of its legibility and present an optimization approach for autonomously generating object and virtual barrier placements.</p> <center> <img src="/blog/assets/img/human_legibility/legible_reach.jpg" alt="Tabletop Manipulation Task" width="300" style="margin-right:30px"/> <img src="/blog/assets/img/human_legibility/navigation_exp.jpg" alt="Warehouse Navigation Task" width="310"/> </center> <p><br/></p> <h2 id="method">Method</h2> <h3 id="legibility-metric">Legibility Metric</h3> <p>First we define the legibility metric that scores an environment configuration in terms of how legibile it is to approach a goal.</p> <center> <img src="/blog/assets/img/human_legibility/env_leg_gt.png"/> </center> <p>Given a ground truth goal $G_{true}$ (blue circle in the example below), we generate a snippet of the human’s trajectory approaching the blue circle. In our experiments we use <a href="https://en.wikipedia.org/wiki/Visibility_graph">visibility graphs</a>. If the most likely goal is not the blue circle, we penalize by a fixed cost $c$.</p> <center> <img src="/blog/assets/img/human_legibility/env_leg_fixed_cost.png" width="500"/> </center> <p>If the robot is able to correctly predict the ground truth goal, we still want to award configurations with more confident predictions. Therefore, the score is the margin function which is the difference between the most likely and second most likely goal probabilities.</p> <center> <img src="/blog/assets/img/human_legibility/margin.png" width="380"/> </center> <p><br/></p> <h3 id="legibility-optimization">Legibility Optimization</h3> <p>Now that we can evaluate the legibility of an environment configuration given a ground truth goal, we present an objective function that evaluates the legibility for a task.</p> <p><br/></p> <center> <img src="/blog/assets/img/human_legibility/leg_opt.png" width="500"/> </center> <p><br/></p> <p>We assume the task $T$ has general precedence constraints. A subtask can only begin after all of its precedence constraints are completed. For example, In an assembly task, you need to finish making the drawers before inserting them into their final positions. In the objective function, we iterate over all possible valid orderings of task $T$, which are the orderings that satisfy precedence constraints. We find the possible ground truth goals at a given state of a task and sum up the legibility scores (EnvLegibility).</p> <h3 id="quality-diversity-for-efficient-search">Quality Diversity for Efficient Search</h3> <p>Now that we know how to evaluate the legibility of a given workspace configuration, we need to search through the space of possible configurations to find the most legible one. This is computationally intractable for most non-trivial applications. We use a quality diversity approach called MAP-Elites <sup id="fnref:map_elites" role="doc-noteref"><a href="#fn:map_elites" class="footnote" rel="footnote">2</a></sup> to efficiently search through the space of possible workspace configurations.</p> <p>The MAP-Elites algorithm consists of two phases, the initialization phase and the improvement phase. In the initialization phase, a random environment is generated and placed in a grid where the dimensions of the grid are some features or behaviors of interest. Here we use the ordering of the cubes in the x-axis and the min distance between cubes. If two environments have the same cell in the grid, the one that has a higher legibility score is stored.</p> <center> <img src="/blog/assets/img/human_legibility/map_elites_init.png" width="300"/> <figcaption>MAP-Elites Initialization</figcaption> </center> <p><br/></p> <p>In the improvement phase, we sample a cell from the grid and run gradient descent. In gradient descent, we first sample new positions for each cube from a Gaussian centered at the cube’s current position. The new configuration may end up in a different cell in the grid. If the cell is not populated or if the new configuration has a better legibility score, then the new configuration is stored in the cell. Then, two cubes are randomly selected and a virtual obstacle of a fixed size is placed between them. We store the new configuration if the legibility score is better.</p> <center> <img src="/blog/assets/img/human_legibility/map_elites_improve.png" width="300"/> <figcaption>MAP-Elites Improvement Phase</figcaption> </center> <p><br/></p> <p>Below is a gif of the gradient descent process.</p> <center> <img src="/blog/assets/img/human_legibility/gradient_descent.gif" width="300"/> </center> <h2 id="demo">Demo</h2> <iframe width="840" height="480" src="https://youtube.com/embed/QVV9SW4lyns" frameborder="0" allow="autoplay" allowfullscreen=""> </iframe> <p><br/> <br/></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:dragan_legibility" role="doc-endnote"> <p>A. D. Dragan, K. C. T. Lee and S. S. Srinivasa, “Legibility and predictability of robot motion,” 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI), Tokyo, Japan, 2013, pp. 301-308, doi: 10.1109/HRI.2013.6483603. <a href="#fnref:dragan_legibility" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:map_elites" role="doc-endnote"> <p>Mouret, Jean-Baptiste, and Jeff Clune. “Illuminating search spaces by mapping elites.” arXiv preprint arXiv:1504.04909 (2015). <a href="#fnref:map_elites" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Improve human legibility by projecting virtual obstacles and rearranging objects in the workspace.]]></summary></entry></feed>