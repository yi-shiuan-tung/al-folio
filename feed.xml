<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://yi-shiuan-tung.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yi-shiuan-tung.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-26T03:46:23+00:00</updated><id>https://yi-shiuan-tung.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Minimizing Entropy for Prediction Problems</title><link href="https://yi-shiuan-tung.github.io/blog/2023/min-entropy/" rel="alternate" type="text/html" title="Minimizing Entropy for Prediction Problems"/><published>2023-12-15T00:00:00+00:00</published><updated>2023-12-15T00:00:00+00:00</updated><id>https://yi-shiuan-tung.github.io/blog/2023/min-entropy</id><content type="html" xml:base="https://yi-shiuan-tung.github.io/blog/2023/min-entropy/"><![CDATA[<p>When we want to be more confident about our predictions for a classification problem, we often use an objective that minimizes the entropy. But why is this not enough? This post will discuss entropy and cross entropy losses.</p> <h2 id="what-is-entropy">What is Entropy?</h2> <p>Entropy (in information theory) is a measure of uncertainty; the higher the entropy, the more uncertain you are. Entropy is defined as</p> \[H(X) = - \sum_{x \in \mathcal{X}} p(x)\text{log} p(x)\] <p>where \(X\) is the discrete random variable that takes values in the alphabet \(\mathcal{X}\) and is distributed according to \(p: \mathcal{X} \rightarrow [0, 1]\). \(-\text{log}p(x)\) is the information of an event \(x\). So entropy \(H\) is the sum of the information for each possible event \(x \in \mathcal{X}\) weighted by the probability of the event \(p(x)\). Rare events (low probability) give more information and have higher values. Another way to think of it is that the entropy of a probability distribution is the optimal number of bits (when using log base 2) required to encode the distribution. When \(p(x)\) is high, we use fewer bits to represent the event \(x\) because we see it more often and it is cheaper to use fewer bits. When \(p(x)\) is low, we use more bits. This is given by the information of the event \(-\text{log}p(x)\).</p> <h2 id="classification-problems">Classification problems</h2> <p>In the context of human goal prediction, we want to train a model that outputs the correct human goal \(x\) given that the model observed some initial human trajectory \(\xi_{S \rightarrow Q}\) that started at point \(S\) and ended at point \(Q\). A human goal can be an object they are reaching towards or some task that they are performing. Suppose the human can reach towards the apple, banana, or grapes (\(\mathcal{X} = \{\text{apple}, \text{banana}, \text{grapes}\}\)), and we have a model \(f\) that outputs a distribution over the likelihood of goals (via neural network with softmax output or a Bayesian classifier). We can get the predicted goal by taking the argmax of the distribution \(\hat{x} = \text{argmax}_{x} f(\xi)\).</p> <center> <img src="/blog/assets/img/reaching_example.png" alt="Reaching Example" width="310"/> </center> <p><a href="https://www.flaticon.com/free-icons/grape" title="grape icons">Grape icons created by Dreamcreateicons - Flaticon</a></p> <p>To train our model to be more certain about its predictions, we can minimize the entropy of the output distribution during training. We can use the following loss function: Given a predicted label \(\hat{x}\) and the true label \(x\), \(\mathcal{L}(x, \hat{x}) = \mathbb{1}\{x = \hat{x}\}H(f(x))+\mathbb{1}\{x != \hat{x}\}c\) for some constant \(c\). This equation penalizes the prediction by \(c\) if the prediction is incorrect and by the entropy if it is correct. \(\mathbb{1}\{q\}\) is the indicator function and evaluates to 1 if \(q\) is true otherwise 0. However, for predictions that are correct, minimizing the entropy may not give you more confident correct predictions. Suppose that the model has the following two predictions for the figure above where the human is reaching for the apple: 1) [0.55, 0.25, 0.2] and 2) [0.45, 0.44, 0.11]. The array corresponds to the goal distribution for apple, banana, and grapes respectively. Intuitively, we prefer the first array because the model is more confident (\(55 \%\)) about the prediction. However, the entropy for 1) is 0.998 and for 2) is 0.963. Minimizing the entropy will move the model outputs closer to 2) [0.45, 0.44, 0.11].</p> <center> <img src="/blog/assets/img/dist1.png" alt="Distribution 1" width="300"/> <img src="/blog/assets/img/dist2.png" alt="Distribution 2" width="300"/> <figcaption style="text-align:justify"> Two possible goal probability distributions. The model is more confident about its prediction on the left, but the entropy is smaller for the distribution on the right. If our objective is to minimize the entropy for correct predictions, we could be pushing the model's output closer to the right distribution.</figcaption> </center> <p><br/></p> <h2 id="connection-to-cross-entropy">Connection to Cross Entropy</h2> <p>A common loss function for classification problems is the cross entropy loss. The cross entropy of distribution \(q\) relative to another distribution \(p\) is defined as</p> \[H(p, q) = - \sum_{x \in \mathcal{X}} p(x)\text{log}q(x)\] <p>Intuitively, it measures the average number of bits needed to encode the actual distribution \(p\) when using the distribution \(q\). We can rewrite \(H(p, q)\) as</p> \[\begin{align} H(p, q) &amp;= - \sum_{x \in \mathcal{X}} p(x)\text{log}q(x)\\ &amp;= -\sum_{x \in \mathcal{X}} p(x) (\frac{\text{log}q(x)}{\text{log}p(x)} \text{log}p(x))\\ &amp;= -\sum_{x \in \mathcal{X}} p(x) \frac{\text{log}q(x)}{\text{log}p(x)} - \sum_{x \in \mathcal{X}} p(x) \text{log}p(x)\\ &amp;= D_{KL}(p||q) + H(p) \end{align}\] <p>The first term is the Kullback-Leibler (KL) divergence which measures how different the distributions \(p\) and \(q\) are, and the second term is the entropy of \(p\). In addition to minimizing entropy, the cross entropy loss minimizes the distance between the predicted and actual distributions which resolves the issue above.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[When we want to be more confident about our predictions for a classification problem, we often use an objective that minimizes the entropy. But why is this not enough? This post will discuss entropy and cross entropy losses.]]></summary></entry><entry><title type="html">Improving Human Legibility in Collaborative Robot Tasks through Augmented Reality and Workspace Preparation</title><link href="https://yi-shiuan-tung.github.io/blog/2023/human-legibility/" rel="alternate" type="text/html" title="Improving Human Legibility in Collaborative Robot Tasks through Augmented Reality and Workspace Preparation"/><published>2023-04-01T00:00:00+00:00</published><updated>2023-04-01T00:00:00+00:00</updated><id>https://yi-shiuan-tung.github.io/blog/2023/human-legibility</id><content type="html" xml:base="https://yi-shiuan-tung.github.io/blog/2023/human-legibility/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>The canonical approach for human robot interaction is to first predict the human motion and then generate a robot plan. This requires a robust and accurate predictions of human behavior. If the human models are inaccurate, the robot may produce unsafe interactions.</p> <p>To achieve more fluent human robot interaction, there has also been work on improving the robot’s expressiveness of its intentions. Dragan et al. developed a formalism for legibility which is the probability of successfully predicting an agent’s goal given an observation of a snippet of its trajectory <sup id="fnref:dragan_legibility" role="doc-noteref"><a href="#fn:dragan_legibility" class="footnote" rel="footnote">1</a></sup>. Subsequent works have shown that robots with legible movements lead to greater task efficiency, trustworthiness, and sense of safety in human robot collaboration. However, there hasn’t been work on looking at how human legibility can improve the robot’s ability to collaborate with humans.</p> <h2 id="our-approach">Our Approach</h2> <p>We improve the robot’s prediction of the human’s goal (aka the human’s legibility) by <strong>projecting virtual barriers in augmented reality and rearranging objects in the workspace</strong>. We present a metric that evaluates an environment configuration in terms of its legibility and present an optimization approach for autonomously generating object and virtual barrier placements.</p> <center> <img src="/blog/assets/img/human_legibility/legible_reach.jpg" alt="Tabletop Manipulation Task" width="300" style="margin-right:30px"/> <img src="/blog/assets/img/human_legibility/navigation_exp.jpg" alt="Warehouse Navigation Task" width="310"/> </center> <p><br/></p> <h2 id="method">Method</h2> <h3 id="legibility-metric">Legibility Metric</h3> <p>First we define the legibility metric that scores an environment configuration in terms of how legibile it is to approach a goal.</p> <center> <img src="/blog/assets/img/human_legibility/env_leg_gt.png"/> </center> <p>Given a ground truth goal $G_{true}$ (blue circle in the example below), we generate a snippet of the human’s trajectory approaching the blue circle. In our experiments we use <a href="https://en.wikipedia.org/wiki/Visibility_graph">visibility graphs</a>. If the most likely goal is not the blue circle, we penalize by a fixed cost $c$.</p> <center> <img src="/blog/assets/img/human_legibility/env_leg_fixed_cost.png" width="500"/> </center> <p>If the robot is able to correctly predict the ground truth goal, we still want to award configurations with more confident predictions. Therefore, the score is the margin function which is the difference between the most likely and second most likely goal probabilities.</p> <center> <img src="/blog/assets/img/human_legibility/margin.png" width="380"/> </center> <p><br/></p> <h3 id="legibility-optimization">Legibility Optimization</h3> <p>Now that we can evaluate the legibility of an environment configuration given a ground truth goal, we present an objective function that evaluates the legibility for a task.</p> <p><br/></p> <center> <img src="/blog/assets/img/human_legibility/leg_opt.png" width="500"/> </center> <p><br/></p> <p>We assume the task $T$ has general precedence constraints. A subtask can only begin after all of its precedence constraints are completed. For example, In an assembly task, you need to finish making the drawers before inserting them into their final positions. In the objective function, we iterate over all possible valid orderings of task $T$, which are the orderings that satisfy precedence constraints. We find the possible ground truth goals at a given state of a task and sum up the legibility scores (EnvLegibility).</p> <h3 id="quality-diversity-for-efficient-search">Quality Diversity for Efficient Search</h3> <p>Now that we know how to evaluate the legibility of a given workspace configuration, we need to search through the space of possible configurations to find the most legible one. This is computationally intractable for most non-trivial applications. We use a quality diversity approach called MAP-Elites <sup id="fnref:map_elites" role="doc-noteref"><a href="#fn:map_elites" class="footnote" rel="footnote">2</a></sup> to efficiently search through the space of possible workspace configurations.</p> <p>The MAP-Elites algorithm consists of two phases, the initialization phase and the improvement phase. In the initialization phase, a random environment is generated and placed in a grid where the dimensions of the grid are some features or behaviors of interest. Here we use the ordering of the cubes in the x-axis and the min distance between cubes. If two environments have the same cell in the grid, the one that has a higher legibility score is stored.</p> <center> <img src="/blog/assets/img/human_legibility/map_elites_init.png" width="300"/> <figcaption>MAP-Elites Initialization</figcaption> </center> <p><br/></p> <p>In the improvement phase, we sample a cell from the grid and run gradient descent. In gradient descent, we first sample new positions for each cube from a Gaussian centered at the cube’s current position. The new configuration may end up in a different cell in the grid. If the cell is not populated or if the new configuration has a better legibility score, then the new configuration is stored in the cell. Then, two cubes are randomly selected and a virtual obstacle of a fixed size is placed between them. We store the new configuration if the legibility score is better.</p> <center> <img src="/blog/assets/img/human_legibility/map_elites_improve.png" width="300"/> <figcaption>MAP-Elites Improvement Phase</figcaption> </center> <p><br/></p> <p>Below is a gif of the gradient descent process.</p> <center> <img src="/blog/assets/img/human_legibility/gradient_descent.gif" width="300"/> </center> <h2 id="demo">Demo</h2> <iframe width="840" height="480" src="https://youtube.com/embed/QVV9SW4lyns" frameborder="0" allow="autoplay" allowfullscreen=""> </iframe> <p><br/> <br/></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:dragan_legibility" role="doc-endnote"> <p>A. D. Dragan, K. C. T. Lee and S. S. Srinivasa, “Legibility and predictability of robot motion,” 2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI), Tokyo, Japan, 2013, pp. 301-308, doi: 10.1109/HRI.2013.6483603. <a href="#fnref:dragan_legibility" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:map_elites" role="doc-endnote"> <p>Mouret, Jean-Baptiste, and Jeff Clune. “Illuminating search spaces by mapping elites.” arXiv preprint arXiv:1504.04909 (2015). <a href="#fnref:map_elites" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="publications"/><summary type="html"><![CDATA[Improve human legibility by projecting virtual obstacles and rearranging objects in the workspace.]]></summary></entry></feed>